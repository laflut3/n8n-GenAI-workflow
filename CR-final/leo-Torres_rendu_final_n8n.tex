% !TEX program = pdflatex
\documentclass[11pt,a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{geometry}
\geometry{margin=2.5cm}

\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue,
  citecolor=blue
}

\usepackage{graphicx}
\usepackage{ifthen}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{xcolor}
\usepackage{amsmath}

\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  numbers=left,
  numberstyle=\tiny,
  tabsize=2
}

\title{Compte rendu — Workflow n8n : Recherche de stages à l’étranger}
\author{Leo Torres \quad — \quad DO3}
\date{\today}

\begin{document}
\maketitle
\tableofcontents
\newpage

% =========================
\section{Contexte et objectifs}
\subsection{Objectif du projet}
Le workflow automatise la recherche d'offres de stage a l'etranger a partir d'une matrice \textit{ville × technologie}. Les resultats sont enrichis par un score de ville (MCP), structures par LLM, puis exportes vers Google Sheets et resumes dans une synthese envoyee sur Discord.

\subsection{Périmètre et hypothèses}
\begin{itemize}
  \item Villes : Berlin, Stockholm.
  \item Technologies : crypto, devops, cybersecurity, iot (8 combinaisons).
  \item Source principale : recherche web via Tavily (1 resultat par requete).
  \item LLM : Google Gemini (gemma-3-4b-it pour extraction, gemma-3-27b-it pour resume, gemini-robotics-er-1.5-preview pour synthese) (à changé en fonction de la dispo des quotas gratuit).
  \item Limites : dependance aux API, champs souvent absents (salaire, remote), redondances d'URLs.
\end{itemize}

% =========================
\section{Architecture du workflow n8n}
\subsection{Vue d’ensemble du workflow}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/workflow_complet.png}
  \caption{Workflow n8n complet.}
\end{figure}

\subsection{Description des étapes}

\subsubsection{Étape 1 — Génération des combinaisons ville × technologie}
Un node Code genere les couples ville x technologie a partir de deux tableaux statiques.
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/node_code_combination.png}
  \caption{Node Code générant les combinaisons ville × technologie.}
\end{figure}

\subsubsection{Étape 2 — Recherche web (Tavily)}
Chaque couple declenche une recherche Tavily avec la requete type \texttt{company <tech> <city> internship}. La recherche est en \texttt{basic}, avec \texttt{max\_results=2}.
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/tavily_search.png}
  \caption{Recherche web via Tavily.}
\end{figure}

\subsubsection{Étape 3 — Enrichissement scoring via MCP}
Le MCP Client appelle l'outil \texttt{find\_city\_score} avec le nom de ville. Si la ville est connue, le MCP renvoie \texttt{sensitivity\_percent} et \texttt{city\_score}; sinon l'outil renvoie une erreur et aucun score n'est ajoute.

\subsubsection{Étape 4 — Extraction d’informations via LLM}
Le LLM retourne un JSON strict, sans texte additionnel, conforme a un schema fixe (company, job\_title, city, country, remote\_policy, contract\_type, salary, currency, duration, application\_deadline, skills, languages, source\_url, source\_title, city\_score). Les valeurs manquantes sont forcees a \texttt{null}.
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/extract_auto_llm.png}
  \caption{Extraction automatique des informations clés via LLM.}
\end{figure}

\subsubsection{Étape 4-bis — Node Code de parsing des extractions}
Le LLM ne renvoie pas toujours un JSON propre (blocs \texttt{```json}, texte parasite, ou tableau/objet selon les cas). Le node Code nettoie d'abord la sortie en supprimant les fences Markdown et en tronquant tout ce qui précède le premier \texttt{\{} ou \texttt{[} et tout ce qui suit le dernier \texttt{\}} ou \texttt{]}. Ensuite, il tente un \texttt{JSON.parse} : si c'est un tableau, chaque entrée devient un item séparé ; si c'est un objet, il est encapsulé dans une liste. Le node normalise enfin le schéma (champs manquants à \texttt{null}, \texttt{skills} et \texttt{languages} en tableaux, \texttt{city\_score} seulement si numérique). Cette étape rend le flux stable et évite les cassures plus loin (filtrage et export).

\subsubsection{Étape 5 — Génération des résumés via LLM}
Un second LLM produit un resume FR de 2 a 3 phrases par offre. Les sorties non-JSON sont nettoyees et parsees dans un node Code.
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/create_summary_llm.png}
  \caption{Génération des résumés via LLM.}
\end{figure}

\subsubsection{Étape 5-bis — Node Code de parsing des résumés}
Le résumé est aussi renvoyé par le LLM sous forme JSON, mais avec les mêmes risques d'irrégularités. Le node Code récupère le texte (souvent \texttt{content.parts[0].text}), supprime les fences \texttt{```json} et nettoie les débordements avant/après le JSON. Il garantit \texttt{1 item en entrée = 1 item en sortie} pour préserver les jointures, récupère \texttt{source\_url} quand il existe, et applique un fallback si le résumé est vide. En cas d'erreur LLM ou de parsing, il sort un item minimal avec un message par défaut, ce qui évite de perdre des offres lors des merges.

Pour ce code de parsing, je me suis aidé de l'IA afin de fiabiliser le nettoyage du JSON et la gestion des cas d'erreur.

\subsubsection{Étape 6 — Filtrage personnalisé}
Le filtrage est minimal : pas de dedoublonnage, seulement un routage selon la presence d'un salaire pour separer deux onglets de stockage.
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/filtre_if.png}
  \caption{Filtrage personnalisé minimal.}
\end{figure}

\subsubsection{Étape 7 — Export des résultats}
Les offres sont ecrites dans Google Sheets (equivalent CSV), dans deux onglets en fonction du filtre (pour les offres ok selon le filtre et pour les autres). Colonnes : company, job\_title, contract\_type, city, country, salary, currency, duration, application\_deadline, skills, languages, source\_url, source\_title, city\_score, summary, remote\_policy.
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/google_sheet_true.png}
  \caption{Export des résultats dans Google Sheets.}
\end{figure}
Pour configurer Google Sheets, il faut d'abord creer un projet dans Google Cloud Console, activer l'API Google Sheets, puis configurer l'ecran de consentement OAuth (type externe) et ajouter son compte en \textit{test user}. Ensuite, creer des identifiants OAuth (client Web ou Bureau), recuperer le fichier JSON, et renseigner dans n8n les champs \textit{Client ID} / \textit{Client Secret} ainsi que l'URL de redirection fournie par n8n. Enfin, authoriser le compte et selectionner le tableur (ou l'ID du fichier) pour que le node puisse ecrire dans l'onglet cible.

\subsubsection{Étape 8 — Synthese finale}
Un LLM genere une synthese lisible avec un titre, le nombre d'offres, un top 3 argumente et des statistiques globales (villes, contrats, remote, salaires).
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/llm_synthese.png}
  \caption{Synthese finale générée par LLM.}
\end{figure}

\subsubsection{Étape 9 — Notification}
La synthese est envoyee sur Discord via webhook. Un node Code tronque le message a 1900 caracteres pour respecter la limite.
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/discord_notification.png}
  \caption{Notification Discord via webhook.}
\end{figure}

% =========================
\section{Exemples de résultats}
\subsection{Offres extraites (5 à 10 exemples)}
Exemple de 5 a 10 offres extraites (toutes les infos dans le JSON) :
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/exemple_offres_json.png}
  \caption{Exemple de 5-10 offres extraites (JSON).}
\end{figure}

\subsection{Fichier CSV exporté (ou capture)}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/export_resultats.png}
  \caption{Aperçu de l’export CSV / Google Sheets.}
\end{figure}

% =========================
\section{Bonus — Méthode de scoring}
\subsection{Méthode utilisée}
Le scoring repose sur un service MCP local qui expose une table statique \texttt{CITY\_SENSITIVITY} (0-100). Pour chaque ville de la matrice, le workflow appelle \texttt{find\_city\_score(city\_name)}. La fonction effectue une recherche insensible a la casse, renvoie \texttt{sensitivity\_percent} si la ville est connue, sinon une erreur et le score est laisse a \texttt{null}. Ce score est ensuite injecte dans le champ \texttt{city\_score} des offres (export CSV et synthese) afin d'apporter un signal rapide d'attractivite sans dependre des donnees extraites.

\subsection{Choix de scoring et justification}
\begin{itemize}
  \item Pertinence : indicateur externe pour comparer rapidement l'attractivite des villes.
  \item Simplicite : table pre-calculée, sans besoin de recalcul complexe.
  \item Robustesse : le score est stable et ne depend pas des offres collectées.
\end{itemize}

\subsection{Évaluation : est-ce que ça fonctionne comme attendu ?}
Pour les villes presentes dans la table, le score est coherent et stable. Les limites viennent des villes absentes ou des libelles atypiques (ex. quartiers, regions). Un enrichissement geographique serait utile.

% =========================
\section{Analyse critique}
\subsection{Difficultés rencontrées}
\begin{itemize}
  \item Choix d'un LLM gratuit : trouver un modele avec un plan gratuit, assez rapide et suffisamment performant pour des extractions fiables.
  \item Parsing et fusion : difficultes a parser les sorties LLM puis a merger les flux pour n'obtenir qu'un seul objet exploitable.
  \item Webhook Discord : configuration et tests pour respecter les contraintes de taille des messages.
  \item Google Sheets : configuration Google Cloud API (OAuth) plus longue que prevu avant de pouvoir ecrire dans un tableur.
\end{itemize}

\subsection{Critères de filtrage choisis et justification}
Le filtrage est volontairement leger pour maximiser la couverture : séparation selon \texttt{salary} present ou non. Cela permet d'identifier rapidement les offres les plus informatives sans exclure trop de candidats.

\subsection{Apports du workflow et axes d’amélioration}
\begin{itemize}
  \item Gain de temps important sur la collecte et la mise en forme des offres.
  \item Ameliorations : augmenter \texttt{max\_results}, ajouter des sources, filtrer par \texttt{city\_score}, ajouter un dedoublonnage, meilleure gestion d'erreurs et retries.
\end{itemize}

% =========================
\section{Conclusion}
Le workflow automatise efficacement la recherche et la synthese d'offres de stage. L'extraction structuree et l'export permettent une analyse rapide, tandis que le scoring MCP apporte un signal supplementaire. Les principaux axes d'amelioration concernent la couverture des sources et la robustesse face aux donnees incompletes.

% =========================
\appendix
\section{Annexes}
\subsection{Prompts LLM (extraits)}
\begin{lstlisting}
Prompt d'extraction (Gemini) :
You are a job-offer information extractor.

STRICT RULES:
- Output ONLY valid JSON. No markdown, no commentary, no surrounding text.
- Do NOT invent. If a value is not explicitly present, use null.
- Empty lists must be [].
- Extract ONE output object per offer in the input array, preserving the same order and length.

OUTPUT FORMAT:
Return a JSON array of objects. Each object must follow EXACTLY this schema:

{
  "company": string|null,
  "job_title": string|null,
  "city": string|null,
  "country": string|null,
  "remote_policy": "onsite"|"hybrid"|"remote"|"unknown"|null,
  "contract_type":   "internship"|"apprenticeship"|"full_time"|"part_time"|"unknown"|null,
  "salary": string|null,
  "currency": string|null,
  "duration": string|null,
  "application_deadline": string|null,
  "skills": string[],
  "languages": string[],
  "source_url": string|null,
  "source_title": string|null,
  "city_score": number|null
}

MAPPING RULES:
- source_url = offer.url if present, else null
- source_title = offer.title if present, else null
- city_score = input.content[0].text.city_score if present (MCP score), else null
- remote_policy:
  - "remote" if clearly remote
  - "hybrid" if clearly hybrid
  - "onsite" if clearly onsite/on-site
  - otherwise "unknown"
- contract_type:
  - "internship" if stage/internship/intern
  - "apprenticeship" if alternance/apprenticeship
  - otherwise "unknown"
- salary: keep original text (e.g. "800 EUR/month", "$22k-$62k")
- currency: "EUR", "USD", "GBP" if clearly identifiable, else null

INPUT:
You will receive a JSON object containing offers in an array field. Use:
- offers = input.result OR input.results (whichever exists)

Input JSON :
{
  "city": string,
  "sensitivity_percent": number,
}

-------------------------------------------------------------------------------------------

Prompt de resume (Gemini) :
Generate a short readable summary for each extracted offer.

STRICT RULES:
- Output ONLY valid JSON. No markdown, no commentary.
- Return a JSON array with the same length/order as the input array.
- Do not include any other keys.

OUTPUT SCHEMA (per item):
{
  "source_url": string|null,
  "summary": string
}

SUMMARY RULES:
- 2 to 3 sentences maximum.
- Include: company (if known), role, location, remote policy, contract type, and salary if present.
- If a field is missing, omit it (do not guess).
- Write the summary in French.

INPUT JSON (array of extracted offers):
[
  {
    "company": "crypto",
    "job_title": "Director of Venture Studio",
    "city": "Berlin",
    "country": "Germany",
    "remote_policy": "unknown",
    "contract_type": "internship",
    "salary": null,
    "currency": null,
    "duration": null,
    "application_deadline": null,
    "skills": [],
    "languages": [],
    "source_url": "https://www.linkedin.com/jobs/crypto-jobs-berlin?countryRedirected=1",
    "source_title": "95 Crypto jobs in Berlin, Berlin, Germany (1 new)",
    "city_score": 9.67
  },
  ...
]

OUTPUT SCHEMA (per item):
{
  "source_url": string|null,
  "summary": string
}

-------------------------------------------------------------------------------------------

Prompt de synthese finale (Gemini) :
Rules:
  - Use only the provided data
  - Do not invent anything
  - The output must be clean, structured, and directly readable by a human

You must write the final answer in French.

Task:
Summarize the internship offers provided in the JSON below.

Produce:
1. Title: "Synthese des offres de stage"
2. Total number of offers
3. Top 3 offers (company, job_title, city, short reason based only on the data)
4. Statistics:
   - Most frequent cities
   - Contract types distribution
   - Remote / onsite / hybrid distribution
   - Salaries if present
5. Short conclusion (2-3 sentences)

Rules:
- Use only the provided data
- Do not invent missing information
- Be concise and structured

Data:

\end{lstlisting}

\end{document}
